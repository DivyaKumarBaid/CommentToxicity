{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow pandas matplotlib sklearn\n",
    "# !pip install tensorflow-gpu\n",
    "#since the tensorflow-gpu is now removed and after tf2.1, the gpu is merged to the tensorflow package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset\n",
    "- data is from kaggle - https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "df = pd.read_csv(\"dataset/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[6]['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[2:]].iloc[6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Batch processing is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text vectorization maps text features to integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into features and target\n",
    "X = df['comment_text']\n",
    "y = df[df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limiting the number of words allowed\n",
    "MAX_FEATURES = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=MAX_FEATURES,\n",
    " output_sequence_length=1800,output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X.values)\n",
    "# X.values convert the X into a numpy array that can be passed as tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = vectorizer(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor flow data pipeline \n",
    "# MCSHBAP - map, cache, shuffle, batch, prefetch from_tensor_slices, list_file \n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,y))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)\n",
    "dataset = dataset.batch(16) #produces batches of 16 datasets\n",
    "dataset = dataset.prefetch(8) #helps preventing bottle neck "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives the batch of 16 samples\n",
    "batch_X,batch_y=dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X.shape\n",
    "# it is 1800 since we limited the size \n",
    "# output_sequence_length=1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating dataset\n",
    "# the split is in ratio of 7:2:1 train:validation:test\n",
    "train = dataset.take(int(len(dataset)*0.7))\n",
    "validation = dataset.skip(int(len(dataset)*0.7)).take(int(len(dataset)*0.2))\n",
    "test = dataset.skip(int(len(dataset)*0.9)).take(int(len(dataset)*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(validation), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train.as_numpy_iterator()\n",
    "train_generator.next()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Deep learning model\n",
    "first layer is embedding - personality test for words\n",
    "\n",
    "### Creating Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional,Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# embedding - for each word we will embedd its personality into 32 features\n",
    "model.add(Embedding(MAX_FEATURES+1,32))\n",
    "# 32 different LSTM units of birdirectional means we can have both forward and backward pass Neural Network Layer to update the weights\n",
    "# and activation function used is tanh since its directed by the tensorflow in gpu computing\n",
    "model.add(Bidirectional(LSTM(32,activation='tanh')))\n",
    "\n",
    "#feature extracot fully connected layers\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "\n",
    "#final layer and have the shape of target\n",
    "model.add(Dense(6,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='BinaryCrossentropy',optimizer=\"Adam\")\n",
    "\n",
    "#BinaryCrossentropy is used since we are using 6 different binary classifier and to reduce loss for cases where one classifier affects another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train,epochs=1,validation_data=validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
